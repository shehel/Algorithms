{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Implementation of a basic neural networks with numpy to get in grips with the backpropagation algorithm. Uses sigmoid activations and mean squared error cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations of backprop\n",
    "Expression for output error\n",
    "$$\\delta^L = \\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial a^L} * \\frac{\\partial a^L}{\\partial z^L}$$\n",
    "\n",
    "Error in the intermediate layers\n",
    "$$\\delta^l = \\frac{\\partial C}{\\partial z^l} = \\frac{\\partial C}{\\partial z^{l+1}}* \\frac {\\partial z^{l+1}}{\\partial z^l}$$\n",
    "<br>\n",
    "Rate of change of cost with respect to weights and biases <br>\n",
    "$$\\frac{\\partial C}{\\partial w^l} = a^{l-1} \\delta^l$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b} = \\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "  $$C = \\frac{1}{2n} \\sum_x \\|y(x)-a^L(x)\\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    #neurons: list of integers defining number of neurons in each layer. \n",
    "    def __init__(self, neurons):\n",
    "        #Neurons defined in a list [6, 4, 3] where the first index is the number of elements in\n",
    "        #the input. 4 and 3 layers in the first and seconda hidden layers respectively.\n",
    "        self.neurons = neurons\n",
    "        self.num_layers = len(neurons)\n",
    "        self.biases = [np.random.randn(num, 1) for num in neurons[1:]]\n",
    "        #Weights are in the format where outp(i) is the i-th neuron in the hidden/outp layer \n",
    "        #and inp(j) is the j-th element in the inp/hidden layer. \n",
    "        self.weights = [np.random.randn(outp, inp) for inp,outp in zip(neurons[:-1] ,neurons[1:])]\n",
    "    \n",
    "    def feedforward(self, test_data):        \n",
    "        activation = test_data.reshape(784, 1)\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activation) + b\n",
    "            activation = sigmoid(z)\n",
    "        return activation\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        len_trn = len(training_data)\n",
    "        n_test = len(test_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, len_trn, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_minibatch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print (\"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print (\"Epoch {0} complete\".format(j))\n",
    "    \n",
    "    def update_minibatch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x,y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nb+dnb for nb, dnb in zip(nabla_w, delta_nabla_w)]\n",
    "            self.weights = [w - eta/len(mini_batch) * nw\n",
    "                            for w, nw in zip(self.weights, nabla_w)]\n",
    "\n",
    "            self.biases = [b - eta/len(mini_batch) * nb\n",
    "                           for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        #_____feedforward______\n",
    "        activation = x.reshape(784, 1)\n",
    "        #list of activations\n",
    "        activations = [activation]\n",
    "        #list of z values\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        #____Backward pass____\n",
    "        #dC/da\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        \n",
    "        #dC/dw and dC/db for last layer\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        #dc/dw and dC/db for every previous layers\n",
    "        for l in range(2, self.num_layers):\n",
    "            self.weights[-l+1].transpose()\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sigmoid_prime(zs[-l])\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def cost_derivative(self, y_est, y_real):\n",
    "        #Derivative of the cost function\n",
    "        return y_est-y_real\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    #The sigmoid function\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    #Derivative of the sigmoid function\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to one hot encode labels\n",
    "def one_hot(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = NN([784, 60, 40, 30, 20, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Path to MNIST in .npz format\n",
    "path = '/home/hearth/.keras/datasets/mnist.npz'\n",
    "data = np.load(path)\n",
    "X_train, y_train, X_test, y_test = data['x_train'], data['y_train'], data['x_test'], data['y_test'] \n",
    "X_train = X_train.reshape(len(X_train), 28*28)\n",
    "X_test = X_test.reshape(len(X_test), 28*28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = [one_hot(j) for j in y_train]\n",
    "y_test = [one_hot(j) for j in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = list(zip(X_train, y_train))\n",
    "test_data = list(zip(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.SGD(training_data, 50, 64, 0.005,\n",
    "            test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(6400, 784)\n",
    "result = [random.randint(0,9) for x in range(0,6400)]\n",
    "training_data = zip(x, result)\n",
    "training_data = list(training_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
